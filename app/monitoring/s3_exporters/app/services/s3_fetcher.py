import boto3
import json
import pandas as pd
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv


load_dotenv()

class BTCDataAWSFetcher:
    """è² è²¬æŠ“å– S3 JSON æª”æ¡ˆã€è¨ˆç®—æ¯æ—¥æŒ‡æ¨™å¹³å‡å€¼ï¼Œä¸¦é©é… MySQL å­˜å„²æ ¼å¼"""

    def __init__(self):
        """åˆå§‹åŒ– S3 é€£ç·š"""
        self.s3 = boto3.client(
            "s3",
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
            region_name=os.getenv("AWS_REGION", "ap-northeast-1")
        )
        self.bucket_name = os.getenv("AWS_S3_BUCKET_NAME", "btc-scraper-storage")

    def _list_s3_files(self):
        """åˆ—å‡º S3 `btc_data_*.json` æª”æ¡ˆ"""
        response = self.s3.list_objects_v2(Bucket=self.bucket_name)
        if "Contents" not in response:
            return []

         # éæ¿¾æ–‡ä»¶åç¨±ï¼Œåªä¿ç•™æ—¥æœŸ <= æ˜¨å¤©çš„æª”æ¡ˆ
        all_file_names = [
            obj["Key"] for obj in response["Contents"] # ç¢ºä¿æ˜¯ YYYYMMDD æ ¼å¼ # éæ¿¾æ¢ä»¶
        ]
        return all_file_names

    def _fetch_s3_data(self, file_names):
        """ä¸‹è¼‰ S3 æª”æ¡ˆä¸¦è½‰æ›ç‚º DataFrame"""
        data_list = []

        for file_key in file_names:
            obj = self.s3.get_object(Bucket=self.bucket_name, Key=file_key)
            file_data = json.loads(obj["Body"].read().decode("utf-8"))
            # è§£æ JSON æ ¼å¼
            record = {
                "trade_date": datetime.strptime(file_data["timestamp"], "%Y/%m/%d %H:%M:%S").strftime("%Y-%m-%d"),
                "funding_rate": float(file_data["funding_rate"]["fundingRate"]) if file_data.get("funding_rate") else None,
                "open_interest": float(file_data["open_interest"]) if file_data.get("open_interest") else None,
                "leverage_ratio": float(file_data["leverage_ratio"]) if file_data.get("leverage_ratio") else None,
                "fear_greed_value": int(file_data["fear_greed_index"]["value"]) if file_data.get("fear_greed_index") else None,
                "fear_greed_cls": file_data["fear_greed_index"]["value_classification"] if file_data.get("fear_greed_index") else None,
                "usdt_supply": float(file_data["usdt_supply"]["circulating_supply"]) if file_data.get("usdt_supply") else None,
                "usdt_market_cap": float(file_data["usdt_supply"]["market_cap"]) if file_data.get("usdt_supply") else None,
                "spot_price": float(file_data["spot_price"]) if file_data.get("spot_price") else None
            }

            data_list.append(record)

        return pd.DataFrame(data_list)
    
    def _aggregate_daily_metrics(self, df):
        """å°åŒä¸€å¤©çš„æ•¸æ“šå–å¹³å‡ï¼Œç”Ÿæˆ `btc_market_metrics` è¡¨æ ¼æ ¼å¼"""
        # æ•¸å€¼æ¬„ä½å–å¹³å‡
        numeric_cols = ["funding_rate", "open_interest", "leverage_ratio", "fear_greed_value", "usdt_supply", "usdt_market_cap"]
        #Pandas çš„ groupby().mean() ä¸éœ€è¦ é¡å¤–çš„ skipna=True åƒæ•¸ï¼Œå› ç‚º mean() æœ¬èº« é è¨­ å°±æœƒå¿½ç•¥ NaN
        df_numeric_avg = df.groupby("trade_date")[numeric_cols].mean().reset_index()

        # æ ¹æ“š `fear_greed_value` å¹³å‡å€¼ä¾†æ±ºå®š `fear_greed_cls`
        df_numeric_avg["fear_greed_cls"] = df_numeric_avg["fear_greed_value"].apply(self._map_fear_greed_classification)
        return df_numeric_avg
    
    def _map_fear_greed_classification(self, value):
        """æ ¹æ“š `fear_greed_index["value"]` ä¾†æ±ºå®š `value_classification`"""
        if value >= 75:
            return "Extreme Greed"
        elif value >= 55:
            return "Greed"
        elif value >= 45:
            return "Neutral"
        elif value >= 25:
            return "Fear"
        else:
            return "Extreme Fear"
        
    def _delete_s3_files(self, file_names):
        """åˆªé™¤å·²è™•ç†çš„ S3 æª”æ¡ˆ"""
        for file_key in file_names:
            self.s3.delete_object(Bucket=self.bucket_name, Key=file_key)
            print(f"ğŸ—‘ï¸ {file_key} å·²åˆªé™¤")

    def process_s3_data(self):
        """ä¸»å‡½æ•¸ï¼šæŠ“å– S3 æ•¸æ“šã€è¨ˆç®—æ¯æ—¥å¹³å‡å€¼"""
        all_file_names = self._list_s3_files()
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        filtered_file_names = [
            obj for obj in all_file_names if obj.startswith("2") and obj.split("_")[0] <= yesterday
        ]

        if not filtered_file_names:
            print("âœ… S3 æ²’æœ‰æ–°æ•¸æ“š")
            return None

        df = self._fetch_s3_data(filtered_file_names)
        if df.empty:
            print("âŒ ç„¡æœ‰æ•ˆæ•¸æ“š")
            return None

        # è¨ˆç®—æ¯æ—¥å¹³å‡å€¼
        daily_metrics = self._aggregate_daily_metrics(df)
        print(f"âœ… è™•ç†å®Œæˆï¼Œå…± {len(daily_metrics)} ç­†æ¯æ—¥æ•¸æ“š")
        
        # åˆªé™¤ S3 ä¸Šå·²è™•ç†çš„æª”æ¡ˆ
        self._delete_s3_files(filtered_file_names)

        return daily_metrics
    
    def get_latest_s3_data(self):
        all_file_names = self._list_s3_files()  # prefix å¯æ ¹æ“šæ—¥æœŸè™•ç†
        if not all_file_names:
            return None  # æ²’æœ‰æª”æ¡ˆ
            # ä¾ç…§ (æ—¥æœŸ, ç·¨è™Ÿ) æ’åº
        def sort_key(key_name):
            parts = key_name.replace(".json", "").split("_")
            date = parts[0]
            index = int(parts[-1])
            return (date, index)
        latest_file_name = sorted(all_file_names, key=sort_key)[-1]  # æœ€å¾Œä¸€å€‹ç‚ºæœ€æ–°
        return self._fetch_s3_data([latest_file_name])

if __name__ == "__main__":
    pass


